# Tokenization
Tokenization is a process of splitting text into meaningful segments

NLP platform: https://www.firstlanguage.in/

Github Code:- https://github.com/codebasics/nlp-tutorials/blob/main/4_tokenization/spacy_tokenizer_tutorial.ipynb

